{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7539019,"sourceType":"datasetVersion","datasetId":4390003}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport traceback\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport shutil\nimport numpy as np\nimport random as  rnd\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Input\n\nfrom termcolor import colored\n\nrnd.seed(32)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-02T12:31:32.189969Z","iopub.execute_input":"2024-02-02T12:31:32.190346Z","iopub.status.idle":"2024-02-02T12:31:35.760542Z","shell.execute_reply.started":"2024-02-02T12:31:32.190313Z","shell.execute_reply":"2024-02-02T12:31:35.759555Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"tmp_txt = '/kaggle/input/text-corpus/shakespeare_data.txt'\nwith open(tmp_txt) as file:\n    text = file.read()","metadata":{"execution":{"iopub.status.busy":"2024-02-02T12:31:49.186913Z","iopub.execute_input":"2024-02-02T12:31:49.187903Z","iopub.status.idle":"2024-02-02T12:31:49.213703Z","shell.execute_reply.started":"2024-02-02T12:31:49.187856Z","shell.execute_reply":"2024-02-02T12:31:49.212446Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"vocab = sorted(set(text))\nvocab.insert(0, '[UNK]')\nvocab.insert(1, '')","metadata":{"execution":{"iopub.status.busy":"2024-02-02T12:32:15.192655Z","iopub.execute_input":"2024-02-02T12:32:15.193100Z","iopub.status.idle":"2024-02-02T12:32:15.270901Z","shell.execute_reply.started":"2024-02-02T12:32:15.193062Z","shell.execute_reply":"2024-02-02T12:32:15.269941Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"n_train = int(0.8*len(text))\nn_val = int(0.1*len(text))\nn_test = len(text)- (n_train+n_val)\n\ntrain_ds = text[:n_train]\nval_ds = text[n_train: n_train+n_val]\ntest_ds = text[n_train+n_val:]\n","metadata":{"execution":{"iopub.status.busy":"2024-02-02T12:32:15.473735Z","iopub.execute_input":"2024-02-02T12:32:15.474061Z","iopub.status.idle":"2024-02-02T12:32:15.482781Z","shell.execute_reply.started":"2024-02-02T12:32:15.474036Z","shell.execute_reply":"2024-02-02T12:32:15.481748Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"len(train_ds), len(val_ds), len(test_ds)","metadata":{"execution":{"iopub.status.busy":"2024-02-02T12:32:18.601988Z","iopub.execute_input":"2024-02-02T12:32:18.602732Z","iopub.status.idle":"2024-02-02T12:32:18.609407Z","shell.execute_reply.started":"2024-02-02T12:32:18.602697Z","shell.execute_reply":"2024-02-02T12:32:18.608486Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(4227103, 528387, 528389)"},"metadata":{}}]},{"cell_type":"code","source":"def sequence_to_tensor(sequence, vocab):\n    if isinstance(sequence, (list)):\n        sequence = '\\n'.join(sequence)\n    chars = tf.strings.unicode_split(sequence, input_encoding='UTF-8')\n    indexes = tf.keras.layers.StringLookup(vocabulary=vocab, mask_token=None)(chars)\n    \n    return indexes","metadata":{"execution":{"iopub.status.busy":"2024-02-02T14:29:19.226408Z","iopub.execute_input":"2024-02-02T14:29:19.227396Z","iopub.status.idle":"2024-02-02T14:29:19.233700Z","shell.execute_reply.started":"2024-02-02T14:29:19.227349Z","shell.execute_reply":"2024-02-02T14:29:19.232702Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"def idx_to_chars(indexes, vocab):\n    chars = tf.keras.layers.StringLookup(vocabulary=vocab, invert=True, mask_token=None)\n    \n    return tf.strings.reduce_join(chars(indexes), axis=-1)","metadata":{"execution":{"iopub.status.busy":"2024-02-02T12:32:22.640049Z","iopub.execute_input":"2024-02-02T12:32:22.640917Z","iopub.status.idle":"2024-02-02T12:32:22.645998Z","shell.execute_reply.started":"2024-02-02T12:32:22.640885Z","shell.execute_reply":"2024-02-02T12:32:22.644950Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def split_input_target(sequence):\n    input_sequence = sequence[:-1]\n    target_sequence = sequence[1:]\n    \n    return input_sequence, target_sequence","metadata":{"execution":{"iopub.status.busy":"2024-02-02T12:32:23.068769Z","iopub.execute_input":"2024-02-02T12:32:23.069718Z","iopub.status.idle":"2024-02-02T12:32:23.074066Z","shell.execute_reply.started":"2024-02-02T12:32:23.069681Z","shell.execute_reply":"2024-02-02T12:32:23.073248Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def prepare_dataset(text, vocab, sequence_length=30, batch_size=64):\n    BUFFER = 10000\n    \n    if isinstance(text, (list)):\n        text = '\\n'.join(text)\n        \n    text_to_tensor = sequence_to_tensor(text, vocab)\n    idx_dataset =  tf.data.Dataset.from_tensor_slices(tensors=text_to_tensor)\n    \n    datagen = idx_dataset.batch(sequence_length +1, drop_remainder=True)\n    dataset_xy = datagen.map(split_input_target)\n    \n    dataset = (\n        dataset_xy\n        .shuffle(BUFFER)\n        .batch(batch_size)\n        .prefetch(tf.data.experimental.AUTOTUNE)\n    )\n    \n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-02-02T12:32:25.709053Z","iopub.execute_input":"2024-02-02T12:32:25.709413Z","iopub.status.idle":"2024-02-02T12:32:25.716247Z","shell.execute_reply.started":"2024-02-02T12:32:25.709385Z","shell.execute_reply":"2024-02-02T12:32:25.715219Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(32)\ntrain_dataset = prepare_dataset(train_ds, sequence_length=100, vocab=vocab, batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2024-02-02T12:32:28.549696Z","iopub.execute_input":"2024-02-02T12:32:28.550060Z","iopub.status.idle":"2024-02-02T12:32:32.644497Z","shell.execute_reply.started":"2024-02-02T12:32:28.550030Z","shell.execute_reply":"2024-02-02T12:32:32.643373Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class GRULM(tf.keras.Model):\n    def __init__(self, vocab_size=256, rnn_units=128, embedding_dim=256):\n        super().__init__(self)\n        \n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n        self.dense = tf.keras.layers.Dense(vocab_size, activation='log_softmax')\n        \n    def call(self, inputs, states=None, return_states=False, training=False):\n        x = inputs\n        x = self.embedding(x, training=training)\n        if states is None:\n            states = self.gru.get_initial_state(x)\n        x, states = self.gru(x, initial_state=states, training=training)\n        x = self.dense(x, training=training)\n        \n        if return_states:\n            return x, states\n        else:\n            return x","metadata":{"execution":{"iopub.status.busy":"2024-02-02T12:32:32.646728Z","iopub.execute_input":"2024-02-02T12:32:32.647131Z","iopub.status.idle":"2024-02-02T12:32:32.656348Z","shell.execute_reply.started":"2024-02-02T12:32:32.647094Z","shell.execute_reply":"2024-02-02T12:32:32.655258Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def compile_model(model):\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    optimizer = tf.keras.optimizers.Adam()\n    \n    model.compile(loss=loss, optimizer=optimizer)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2024-02-02T12:32:32.657479Z","iopub.execute_input":"2024-02-02T12:32:32.657753Z","iopub.status.idle":"2024-02-02T12:32:32.666969Z","shell.execute_reply.started":"2024-02-02T12:32:32.657728Z","shell.execute_reply":"2024-02-02T12:32:32.666191Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def log_perplexity(preds, target):\n    \n    PADDING_ID = 1\n    \n    log_p = np.sum(tf.one_hot(target,preds.shape[-1]) * preds, axis= -1)\n    non_pad = 1.0 - np.equal(target, PADDING_ID)\n    log_p = log_p * non_pad\n    log_ppx = np.sum(log_p, axis=-1) / np.sum(non_pad, axis=-1)\n    log_ppx = np.mean(log_ppx)\n        \n    return -log_ppx","metadata":{"execution":{"iopub.status.busy":"2024-02-02T12:32:36.174343Z","iopub.execute_input":"2024-02-02T12:32:36.175280Z","iopub.status.idle":"2024-02-02T12:32:36.181115Z","shell.execute_reply.started":"2024-02-02T12:32:36.175241Z","shell.execute_reply":"2024-02-02T12:32:36.180067Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Length of the vocabulary in StringLookup Layer\nvocab_size = len(vocab)\n\n# The embedding dimension\nembedding_dim = 256\n\n# RNN layers\nrnn_units = 512\n\nmodel = GRULM(\n    vocab_size=vocab_size,\n    embedding_dim=embedding_dim,\n    rnn_units = rnn_units)","metadata":{"execution":{"iopub.status.busy":"2024-02-02T12:56:41.820926Z","iopub.execute_input":"2024-02-02T12:56:41.821855Z","iopub.status.idle":"2024-02-02T12:56:41.836083Z","shell.execute_reply.started":"2024-02-02T12:56:41.821817Z","shell.execute_reply":"2024-02-02T12:56:41.835260Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 64\nmodel.build(input_shape=(BATCH_SIZE, 100))\nmodel.call(inputs=Input(shape=(100)))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-02-02T12:32:39.133348Z","iopub.execute_input":"2024-02-02T12:32:39.133744Z","iopub.status.idle":"2024-02-02T12:32:39.847823Z","shell.execute_reply.started":"2024-02-02T12:32:39.133713Z","shell.execute_reply":"2024-02-02T12:32:39.846909Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Model: \"grulm\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (None, 100, 256)          20992     \n                                                                 \n gru (GRU)                   [(None, 100, 512),        1182720   \n                              (None, 512)]                       \n                                                                 \n dense (Dense)               (None, 100, 82)           42066     \n                                                                 \n=================================================================\nTotal params: 1245778 (4.75 MB)\nTrainable params: 1245778 (4.75 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"EPOCHS = 10\nmodel = compile_model(model)\nhistory = model.fit(train_dataset, epochs=EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2024-02-02T12:36:39.940729Z","iopub.execute_input":"2024-02-02T12:36:39.941621Z","iopub.status.idle":"2024-02-02T12:40:05.533175Z","shell.execute_reply.started":"2024-02-02T12:36:39.941582Z","shell.execute_reply":"2024-02-02T12:40:05.532264Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Epoch 1/10\n1308/1308 [==============================] - 23s 15ms/step - loss: 1.4339\nEpoch 2/10\n1308/1308 [==============================] - 20s 14ms/step - loss: 1.3434\nEpoch 3/10\n1308/1308 [==============================] - 21s 15ms/step - loss: 1.3003\nEpoch 4/10\n1308/1308 [==============================] - 20s 14ms/step - loss: 1.2729\nEpoch 5/10\n1308/1308 [==============================] - 20s 14ms/step - loss: 1.2532\nEpoch 6/10\n1308/1308 [==============================] - 20s 14ms/step - loss: 1.2379\nEpoch 7/10\n1308/1308 [==============================] - 20s 14ms/step - loss: 1.2248\nEpoch 8/10\n1308/1308 [==============================] - 20s 14ms/step - loss: 1.2143\nEpoch 9/10\n1308/1308 [==============================] - 20s 14ms/step - loss: 1.2053\nEpoch 10/10\n1308/1308 [==============================] - 20s 14ms/step - loss: 1.1972\n","output_type":"stream"}]},{"cell_type":"code","source":"output_dir = './first-nlp-model/'\n\ntry:\n    shutil.rmtree(output_dir)\nexcept OSError as e:\n    pass\n\nmodel.save_weights(output_dir)","metadata":{"execution":{"iopub.status.busy":"2024-02-02T13:11:54.537891Z","iopub.execute_input":"2024-02-02T13:11:54.538696Z","iopub.status.idle":"2024-02-02T13:11:54.593457Z","shell.execute_reply.started":"2024-02-02T13:11:54.538661Z","shell.execute_reply":"2024-02-02T13:11:54.592695Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(vocab)\nembedding_dim = 256\nrnn_units = 512\n\nmodel = GRULM(vocab_size=vocab_size, embedding_dim=embedding_dim, rnn_units = rnn_units)\nmodel.build(input_shape=(100, vocab_size))\nmodel.load_weights('./first-nlp-model/')","metadata":{"execution":{"iopub.status.busy":"2024-02-02T13:11:55.646224Z","iopub.execute_input":"2024-02-02T13:11:55.647263Z","iopub.status.idle":"2024-02-02T13:11:56.011230Z","shell.execute_reply.started":"2024-02-02T13:11:55.647197Z","shell.execute_reply":"2024-02-02T13:11:56.010280Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7e1a7393d270>"},"metadata":{}}]},{"cell_type":"code","source":"full_dataset = train_dataset.concatenate(val_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-02-02T12:55:29.549797Z","iopub.execute_input":"2024-02-02T12:55:29.550683Z","iopub.status.idle":"2024-02-02T12:55:29.561119Z","shell.execute_reply.started":"2024-02-02T12:55:29.550643Z","shell.execute_reply":"2024-02-02T12:55:29.560335Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 10\nmodel = compile_model(model)\nhistory = model.fit(full_dataset, epochs=EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2024-02-02T12:56:45.649975Z","iopub.execute_input":"2024-02-02T12:56:45.650879Z","iopub.status.idle":"2024-02-02T13:00:50.161420Z","shell.execute_reply.started":"2024-02-02T12:56:45.650840Z","shell.execute_reply":"2024-02-02T13:00:50.160586Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Epoch 1/10\n1390/1390 [==============================] - 25s 15ms/step - loss: 1.7707\nEpoch 2/10\n1390/1390 [==============================] - 22s 15ms/step - loss: 1.4030\nEpoch 3/10\n1390/1390 [==============================] - 22s 15ms/step - loss: 1.3346\nEpoch 4/10\n1390/1390 [==============================] - 22s 15ms/step - loss: 1.3005\nEpoch 5/10\n1390/1390 [==============================] - 22s 15ms/step - loss: 1.2787\nEpoch 6/10\n1390/1390 [==============================] - 22s 15ms/step - loss: 1.2625\nEpoch 7/10\n1390/1390 [==============================] - 22s 15ms/step - loss: 1.2503\nEpoch 8/10\n1390/1390 [==============================] - 23s 15ms/step - loss: 1.2402\nEpoch 9/10\n1390/1390 [==============================] - 23s 15ms/step - loss: 1.2318\nEpoch 10/10\n1390/1390 [==============================] - 22s 15ms/step - loss: 1.2247\n","output_type":"stream"}]},{"cell_type":"code","source":"test_dataset = prepare_dataset(test_ds, sequence_length=100, vocab=vocab, batch_size=64)","metadata":{"execution":{"iopub.status.busy":"2024-02-02T13:07:57.888971Z","iopub.execute_input":"2024-02-02T13:07:57.889882Z","iopub.status.idle":"2024-02-02T13:07:58.164921Z","shell.execute_reply.started":"2024-02-02T13:07:57.889842Z","shell.execute_reply":"2024-02-02T13:07:58.164073Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"for x, target in test_dataset.take(1):\n    i=5\n    print(idx_to_chars(x[i],vocab).numpy())\n    \n    preds, status = model(x, training=False, states=None, return_states=True)\n    \n    sampled_indices = tf.math.argmax(preds[i], axis=-1)\n    print(idx_to_chars(sampled_indices, vocab).numpy())\n    print(idx_to_chars(target[i], vocab).numpy())\n","metadata":{"execution":{"iopub.status.busy":"2024-02-02T14:29:30.929808Z","iopub.execute_input":"2024-02-02T14:29:30.930795Z","iopub.status.idle":"2024-02-02T14:29:31.727879Z","shell.execute_reply.started":"2024-02-02T14:29:30.930744Z","shell.execute_reply":"2024-02-02T14:29:31.726883Z"},"trusted":true},"execution_count":106,"outputs":[{"name":"stdout","text":"b' power, and so stand aloof for more serious\\n\\twooing. But I protest to thee, pretty one, my\\n\\tauthorit'\nb'truers\\nand to dhrnd usonf,mor tere.ttrvous,\\tTirdng  Wut t wratest th she   arosty lnes\\nay \\tfrnhority'\nb'power, and so stand aloof for more serious\\n\\twooing. But I protest to thee, pretty one, my\\n\\tauthority'\n","output_type":"stream"}]},{"cell_type":"code","source":"eval_ids = sequence_to_tensor(test_ds, vocab)\ninput_ids, target_ids = split_input_target(eval_ids)\n\npreds, status = model(tf.expand_dims(input_ids, 0), training=False, states=None, return_states=True)\n\n#Get the log perplexity\nlog_ppx = log_perplexity(preds, tf.expand_dims(target_ids, 0))\nprint(f'log perplexity: {log_ppx}\\nperplexity:  {np.exp(log_ppx)}')","metadata":{"execution":{"iopub.status.busy":"2024-02-02T16:45:08.660491Z","iopub.execute_input":"2024-02-02T16:45:08.660927Z","iopub.status.idle":"2024-02-02T16:45:15.664341Z","shell.execute_reply.started":"2024-02-02T16:45:08.660892Z","shell.execute_reply":"2024-02-02T16:45:15.663257Z"},"trusted":true},"execution_count":172,"outputs":[{"name":"stdout","text":"log perplexity: 1.3044070955527076\nperplexity:  3.685503294490197\n","output_type":"stream"}]},{"cell_type":"code","source":"def temperature_random_sampling(log_probs, temperature=1.0):\n   \n    u = tf.random.uniform(minval=1e-6, maxval=1.0 - 1e-6, shape=log_probs.shape)\n    \n    # Apply the Gumbel distribution transformation for randomness\n    g = -tf.math.log(-tf.math.log(u))\n    \n    # Adjust the logits with the temperature and choose the character with the highest score\n    return tf.math.argmax(log_probs + g * temperature, axis=-1)","metadata":{"execution":{"iopub.status.busy":"2024-02-02T13:18:17.494799Z","iopub.execute_input":"2024-02-02T13:18:17.495474Z","iopub.status.idle":"2024-02-02T13:18:17.501684Z","shell.execute_reply.started":"2024-02-02T13:18:17.495437Z","shell.execute_reply":"2024-02-02T13:18:17.500764Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"class GenerativeModel(tf.keras.Model):\n    def __init__(self, model, vocab, temperature=1.0):\n        super().__init__()\n        self.model = model\n        self.vocab = vocab\n        self.temperature = temperature\n        \n        \n    \n    @tf.function\n    def generate_one_step(self, inputs, states=None):\n        input_idx = sequence_to_tensor(inputs, vocab)\n        \n        predicted_logits, states = self.model(\n            input_idx, states=states, return_states=True\n        )\n        \n        predicted_logits = predicted_logits[0,-1,:]\n        \n        predicted_idx = temperature_random_sampling(\n            predicted_logits, self.temperature\n        )\n        \n        predicted_chars = idx_to_chars([predicted_idx], vocab)\n        \n        return tf.expand_dims(predicted_chars, 0), states\n    \n    \n    \n    def generate_n_steps(self, prefix, n_steps):\n        \n        states = None\n        next_char = tf.constant([prefix])\n        result = [next_char]\n        \n        for n in range(n_steps):\n            next_char, states = self.generate_one_step(next_char, states=states)\n            result.append(next_char)\n            \n        return tf.strings.join(result)[0].numpy().decode('utf-8')","metadata":{"execution":{"iopub.status.busy":"2024-02-02T16:31:35.018546Z","iopub.execute_input":"2024-02-02T16:31:35.018955Z","iopub.status.idle":"2024-02-02T16:31:35.029493Z","shell.execute_reply.started":"2024-02-02T16:31:35.018922Z","shell.execute_reply":"2024-02-02T16:31:35.028635Z"},"trusted":true},"execution_count":152,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(32)\n\ngen = GenerativeModel(model, vocab, temperature=0.45)\n\nprint(gen.generate_n_steps(n_steps=500, prefix='Dear'),'\\n','_'*30)","metadata":{"execution":{"iopub.status.busy":"2024-02-02T16:36:27.358866Z","iopub.execute_input":"2024-02-02T16:36:27.359254Z","iopub.status.idle":"2024-02-02T16:36:29.515182Z","shell.execute_reply.started":"2024-02-02T16:36:27.359223Z","shell.execute_reply":"2024-02-02T16:36:29.514254Z"},"trusted":true},"execution_count":171,"outputs":[{"name":"stdout","text":"Dear the stakes and sack and promise of the same.\n\nLUCETTA\tI do not take my lord and all these fiends,\n\tThat she will do not the better have the law\n\tAnd seek the bolt under the heart.\n\n\t[Exit PROTEUS]\n\n\tHow now, my lord, I will perceive excellent\n\tof the stars of a man as we are but second to his country.\n\n\t[Exit POLONIUS]\n\n\tLook, how now, kinsman! no more than the matter?\n\nEDGAR\tThe soul of death I will not see the way\n\tTo the king is at home, the sea, and for the\n\tking of perjury that we did not  \n ______________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}